import kafka from '../configs/kafkaConfig.js';
import redis from '../configs/redisConfig.js';
import 'dotenv/config';
import { logMessage, logBatchFailure, logBatchSuccess } from '../utils/logger.js';
import { releaseLock, acquireLock } from '../utils/helper.js'; // ƒê·∫£m b·∫£o import ƒë√∫ng
import fs from 'fs';
import path from 'path';
import { startWorkerUpdater } from '../utils/workerManager.js';

const consumer = kafka.consumer({
	groupId: 'master-group',
	metadataMaxAge: 60000, // 1 ph√∫t
	allowAutoTopicCreation: true,
	retry: {
		initialRetryTime: 100,
		retries: 8,
	},
	sessionTimeout: 60000,
	heartbeatInterval: 10000,
	rebalanceTimeout: 60000,
});

const getChunkById = async (chunkId) => {
	// Chunk id = vietnam_ips_1_chunk_1.txt, -> fileName = vietnam_ips_1.txt
	const fileName = chunkId.split('_chunk_')[0] + '.txt';
	const DATA_PATH = process.env.CLIENT_DATA_PATH + '/input';
	const filePath = path.join(DATA_PATH, fileName);
	const lines = fs
		.readFileSync(filePath, 'utf-8')
		.split('\n')
		.filter((line) => line.trim());

	const chunkIndex = chunkId.split('_chunk_')[1].split('.')[0];
	const chunkSizeRaw = await redis.get('numBatches');
	const chunkSize = parseInt(chunkSizeRaw, 10) || 500;
	const start = chunkIndex * chunkSize;
	return lines.slice(start, start + chunkSize);
}

// --- Worker Timeout Check --- (Gi·ªØ nguy√™n ho·∫∑c c·∫£i ti·∫øn n·∫øu c·∫ßn)
const checkWorkerStatus = () => {
	console.log('‚è±Ô∏è Starting worker status monitor...');
	const intervalId = setInterval(async () => {
		// L∆∞u intervalId ƒë·ªÉ c√≥ th·ªÉ clear
		try {
			const workers = await redis.hgetall('worker:status');
			const now = Date.now();

			if (Object.keys(workers).length === 0) {
				console.log("Monitor: No workers registered.");
				return;
			}

			for (const [workerId, status] of Object.entries(workers)) {
				if (status === '1') continue; // B·ªè qua worker ƒëang s·∫µn s√†ng

				const batchInfoRaw = await redis.hget('worker:batchInfo', workerId);
				// console.log(`Monitor check ${workerId}: Status=${status}, BatchInfo=${batchInfoRaw}`);

				if (!batchInfoRaw) {
					console.warn(
						`‚ö†Ô∏è Worker ${workerId} is busy (0) but has no batchInfo. Resetting.`
					);
					await redis.hset('worker:status', workerId, '1'); // Reset v·ªÅ s·∫µn s√†ng
					await releaseLock(workerId); // C·ªë g·∫Øng gi·∫£i ph√≥ng lock ph√≤ng tr∆∞·ªùng h·ª£p c√≤n s√≥t
					continue;
				}

				let batchInfo;
				try {
					batchInfo = JSON.parse(batchInfoRaw);
					if (
						!batchInfo ||
						typeof batchInfo.total !== 'number' ||
						typeof batchInfo.assignedAt !== 'number'
					) {
						console.warn(
							`‚ö†Ô∏è Worker ${workerId} has invalid batchInfo: ${batchInfoRaw}. Resetting.`
						);
						await redis.hset('worker:status', workerId, '1');
						await releaseLock(workerId);
						await redis.hdel('worker:batchInfo', workerId); // X√≥a th√¥ng tin batch l·ªói
						continue;
					}
				} catch (e) {
					console.warn(
						`‚ö†Ô∏è Worker ${workerId} failed to parse batchInfo: ${batchInfoRaw}. Resetting. Error: ${e.message}`
					);
					await redis.hset('worker:status', workerId, '1');
					await releaseLock(workerId);
					await redis.hdel('worker:batchInfo', workerId);
					continue;
				}

				const estimatedProcessingTime = (process.env.WORKER_TIMEOUT || 80) * 1000;
				const timeSinceAssigned = now - batchInfo.assignedAt;

				// Ki·ªÉm tra th√™m lastSeen ƒë·ªÉ ch·∫Øc ch·∫Øn worker c√≤n ho·∫°t ƒë·ªông
				const lastSeenRaw = await redis.get(`lastSeen:${workerId}`);
				const lastSeenTime = lastSeenRaw ? parseInt(lastSeenRaw, 10) : 0;
				const timeSinceLastSeen = lastSeenTime > 0 ? now - lastSeenTime : Infinity; // N·∫øu ch∆∞a th·∫•y -> coi nh∆∞ v√¥ h·∫°n

				// Coi worker l√† timeout n·∫øu th·ªùi gian k·ªÉ t·ª´ khi g√°n V∆Ø·ª¢T QU√Å th·ªùi gian ∆∞·ªõc t√≠nh
				// V√Ä th·ªùi gian k·ªÉ t·ª´ l·∫ßn cu·ªëi th·∫•y ho·∫°t ƒë·ªông c≈©ng V∆Ø·ª¢T QU√Å timeout (ho·∫∑c ch∆∞a th·∫•y bao gi·ªù)
				if (
					timeSinceAssigned > estimatedProcessingTime ||
					timeSinceLastSeen > estimatedProcessingTime
				) {
					console.warn(
						`‚ö†Ô∏è Worker ${workerId} timeout detected! Assigned ${timeSinceAssigned / 1000
						}s ago, last seen ${timeSinceLastSeen / 1000}s ago. Resetting...`
					);
					logMessage(`Worker ${workerId} timeout. Resetting.`);
					// X√≥a th√¥ng tin li√™n quan ƒë·∫øn worker n√†y
					// Skip batch id n√†y, ghi l·∫°i l·ªói v√†o log
					const multi = redis.multi();
					multi.hdel('worker:status', workerId);
					multi.hdel('worker:partition', workerId);
					multi.hdel('worker:batchInfo', workerId);
					multi.del(`lastSeen:${workerId}`);
					multi.hdel('worker:processing', batchInfo.batchId); // X√≥a ti·∫øn tr√¨nh c·ªßa batchId n·∫øu c√≥
					console.log(`üßπ Cleaned up timeout worker ${workerId}.`);
					await multi.exec();
					await releaseLock(workerId); // Quan tr·ªçng: gi·∫£i ph√≥ng lock
					logBatchFailure({
						workerId,
						batchId: batchInfo.batchId,
					}, 'Worker timeout');

					// Th√™m l·∫°i chunk v√†o queue
					const chunk = await getChunkById(batchInfo.batchId);
					await redis.rpush('master:fileChunks', JSON.stringify({ chunkId: batchInfo.batchId, chunk }));
					console.log(`üîÑ Added chunk ${batchInfo.batchId} back to queue, size: ${chunk.length}`);
				}
			}
		} catch (error) {
			console.error('‚ùå Error in worker status monitor:', error);
		}
	}, 10000); // Ch·∫°y m·ªói 10 gi√¢y

	return () => clearInterval(intervalId); // Tr·∫£ v·ªÅ h√†m ƒë·ªÉ d·ª´ng interval
};


export const runConsumer = async () => {
	let stopMonitoring = null;
	let stopWorkerUpdater = null; // Th√™m bi·∫øn ƒë·ªÉ gi·ªØ h√†m d·ª´ng worker updater

	try {
		await consumer.connect();
		console.log('‚úÖ Consumer connected');

		await consumer.subscribe({
			topics: [
				process.env.KAFKA_TOPIC_NAME_WORKER_FREE,
				process.env.KAFKA_TOPIC_NAME_WORKER,
			],
			fromBeginning: false,
		});
		console.log(
			`üëÇ Consumer subscribed to topics: ${process.env.KAFKA_TOPIC_NAME_WORKER}, ${process.env.KAFKA_TOPIC_NAME_WORKER_FREE}`
		);

		await consumer.run({
			eachMessage: async ({ topic, partition, message }) => {
				console.log(`\nüì© Received message on topic "${topic}", partition ${partition}`);
				let data = JSON.parse(message.value.toString());

				// --- X·ª≠ l√Ω Worker ƒëƒÉng k√Ω ho·∫∑c b√°o s·∫µn s√†ng ---
				if (topic === process.env.KAFKA_TOPIC_NAME_WORKER_FREE) {
					const { id: workerId, status: workerStatus } = data;
					if (!workerId || !workerStatus) {
						console.warn('‚ö†Ô∏è Received invalid WORKER_FREE message:', data);
						return;
					}

					// ƒê∆∞a v√†o h√†ng ƒë·ª£i Redis n·∫øu l√† c·∫≠p nh·∫≠t partition
					if (workerStatus === 'new' && data.partitions) {
						// Thay v√¨ c·ªë g·∫Øng lock, l∆∞u th√¥ng tin v√†o Redis ƒë·ªÉ x·ª≠ l√Ω sau
						const updateData = {
							partitions: JSON.stringify(data.partitions),
							timestamp: Date.now(),
							status: workerStatus
						};

						await redis.hmset(`worker:pending:update:${workerId}`, updateData);
						await redis.set(`worker:needs:update:${workerId}`, '1', 'EX', 600);

						console.log(`üìù Queued partition update for worker ${workerId}`);

						// V·∫´n c·∫≠p nh·∫≠t lastSeen ƒë·ªÉ bi·∫øt worker c√≤n ho·∫°t ƒë·ªông
						await redis.set(`lastSeen:${workerId}`, Date.now(), 'EX', 60 * 5);
						return;
					}

					// X·ª≠ l√Ω c√°c tr·∫°ng th√°i kh√°c b√¨nh th∆∞·ªùng (nh∆∞ done)
					const hasLock = await acquireLock(workerId, 5000);
					if (!hasLock) {
						console.log(`‚ö†Ô∏è Cannot acquire lock for worker ${workerId} - will queue update`);

						// ƒê∆∞a v√†o h√†ng ƒë·ª£i n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c lock
						const updateData = {
							status: workerStatus,
							timestamp: Date.now()
						};

						await redis.hmset(`worker:pending:update:${workerId}`, updateData);
						await redis.set(`worker:needs:update:${workerId}`, '1', 'EX', 600);
						return;
					}

					try {
						if (workerStatus === 'done') {
							const multi = redis.multi();
							multi.hset('worker:status', workerId, '1');
							await multi.exec();
						}
						// 'new' ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü tr√™n
					} finally {
						await releaseLock(workerId);
					}
				}
				// --- X·ª≠ l√Ω Worker b√°o c√°o ti·∫øn tr√¨nh ---
				else {
					const {
						id: workerId,
						batchId,
						processing,
						total,
					} = data;

					// --- Validate data ---
					if (
						!workerId ||
						!batchId ||
						processing === undefined ||
						total === undefined
					) {
						console.warn('‚ö†Ô∏è Received invalid WORKER progress message:', data);
						return;
					}
					const processedCount = parseInt(processing, 10);
					const totalCount = parseInt(total, 10);
					if (isNaN(processedCount) || isNaN(totalCount)) {
						console.error(
							`‚ùå Invalid processing/total attributes for batch ${batchId} from worker ${workerId}:`,
							data
						);
						return;
					}

					const multi = redis.multi();
					// --- Update last seen time ---
					multi.set(`lastSeen:${workerId}`, Date.now(), 'EX', 60 * 5); // C·∫≠p nh·∫≠t v√† t·ª± h·∫øt h·∫°n sau 5 ph√∫t n·∫øu kh√¥ng c√≥ c·∫≠p nh·∫≠t m·ªõi

					// --- Log progress ---
					console.log(
						`${workerId} progress ${batchId}: ${processedCount}/${totalCount}`
					);
					logMessage(
						`${workerId} progress ${batchId}: ${processedCount}/${totalCount}`
					);
					multi.hset('worker:processing', batchId, processedCount);
					// --- Check if batch is completed ---
					if (processedCount === totalCount) {
						console.log(
							`‚úÖ Worker ${workerId} completed batch ${batchId} (${processedCount}/${totalCount}).`
						);
						logBatchSuccess({
							workerId,
							batchId,
							processedCount,
							totalCount,
						});
						multi.hdel('worker:batchInfo', workerId); // X√≥a th√¥ng tin batch ƒë√£ xong
						console.log(
							`   -> Worker ${workerId} status set to 1 (Ready) and lock released.`
						);
					}
					await multi.exec();
				}
			},
		});

		// Kh·ªüi ƒë·ªông c√°c monitoring
		stopMonitoring = checkWorkerStatus();
		stopWorkerUpdater = startWorkerUpdater(); // Kh·ªüi ƒë·ªông worker updater

		console.log('‚è≥ Consumer is running. Waiting for messages...');
	} catch (error) {
		console.error('‚ùå Fatal error in consumer:', error);
		if (stopMonitoring) stopMonitoring();
		if (stopWorkerUpdater) stopWorkerUpdater(); // D·ª´ng worker updater n·∫øu c√≥ l·ªói
	}

	// X·ª≠ l√Ω t√≠n hi·ªáu d·ª´ng cho consumer
	const errorTypes = ['unhandledRejection', 'uncaughtException'];
	const signalTraps = ['SIGTERM', 'SIGINT', 'SIGUSR2'];

	errorTypes.forEach((type) => {
		process.on(type, async (err) => {
			try {
				console.log(`üî• Process ${type}: ${err.message}`);
				if (stopMonitoring) stopMonitoring();
				if (stopWorkerUpdater) stopWorkerUpdater(); // D·ª´ng worker updater khi c√≥ l·ªói
				await consumer.disconnect();
				console.log('Consumer disconnected on error.');
				process.exit(1);
			} catch (_) {
				process.exit(1);
			}
		});
	});

	signalTraps.forEach((type) => {
		process.once(type, async () => {
			try {
				console.log(`‚úã Signal ${type} received. Shutting down consumer...`);
				if (stopMonitoring) stopMonitoring();
				if (stopWorkerUpdater) stopWorkerUpdater(); // D·ª´ng worker updater khi shutdown
				await consumer.disconnect();
				console.log('Consumer disconnected gracefully.');
			} finally {
				process.kill(process.pid, type); // ƒê·∫£m b·∫£o process k·∫øt th√∫c ƒë√∫ng c√°ch
			}
		});
	});
};
