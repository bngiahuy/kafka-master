import 'dotenv/config';
import kafka from '../configs/kafkaConfig.js';
import fs from 'fs';
import path from 'path';
import redis from '../configs/redisConfig.js';
import { randomUUID } from 'crypto';
const producer = kafka.producer();

const BATCH_DIR = './input_ip_data';
const BATCH_FILES = fs.readdirSync(BATCH_DIR).filter((f) => f.endsWith('.txt'));
let batchQueue = [...BATCH_FILES];

export const assignBatches = async () => {
	// Ti·∫øp t·ª•c x·ª≠ l√Ω cho ƒë·∫øn khi h√†ng ƒë·ª£i file tr·ªëng
	while (batchQueue.length > 0) {
		// --- L·∫•y file ti·∫øp theo t·ª´ h√†ng ƒë·ª£i ---
		const currentBatchFile = batchQueue.shift();
		const batchIdBase = path.basename(currentBatchFile, '.txt') || randomUUID();
		const filePath = path.join(BATCH_DIR, currentBatchFile);
		const lines = fs
			.readFileSync(filePath, 'utf-8')
			.split('\n')
			.map((line) => line.trim())
			.filter(Boolean);

		// --- L·∫•y chunkSize t·ª´ Redis (m·∫∑c ƒë·ªãnh 1000 n·∫øu parse l·ªói) ---
		const chunkSizeRaw = await redis.get('numBatches');
		const chunkSize = parseInt(chunkSizeRaw, 10) || 1000;

		// --- Chia file th√†nh c√°c chunk ---
		const fileChunks = [];
		for (let i = 0; i < lines.length; i += chunkSize) {
			const chunk = lines.slice(i, i + chunkSize);
			const chunkId = `${batchIdBase}_${i / chunkSize}`;
			fileChunks.push({ chunkId, chunk });
		}

		console.log(`\nB·∫Øt ƒë·∫ßu x·ª≠ l√Ω file: ${currentBatchFile}`);
		console.log(`  -> T·ªïng s·ªë chunk: ${fileChunks.length}`);

		// --- G·ª≠i song song c√°c chunk cho c√°c worker s·∫µn s√†ng ---
		const chunkPromises = fileChunks.map(async ({ chunkId, chunk }) => {
			// L·∫∑p v√¥ h·∫°n ƒë·∫øn khi t√¨m ƒë∆∞·ª£c m·ªôt worker s·∫µn s√†ng (status === '1')
			let chosenWorker = null;
			while (!chosenWorker) {
				// L·∫•y danh s√°ch worker v√† tr·∫°ng th√°i
				const allWorkers = await redis.hgetall('worker:status');
				// L·ªçc ra c√°c worker s·∫µn s√†ng
				const readyWorkers = Object.entries(allWorkers)
					.filter(([_, status]) => status === '1')
					.map(([id]) => id);

				if (readyWorkers.length > 0) {
					// Ch·ªçn ng·∫´u nhi√™n 1 worker trong s·ªë c√°c worker s·∫µn s√†ng
					const randomIndex = Math.floor(Math.random() * readyWorkers.length);
					chosenWorker = readyWorkers[randomIndex];

					// ƒê·∫∑t tr·∫°ng th√°i worker th√†nh "busy" ƒë·ªÉ tr√°nh b·ªã ch·ªçn l·∫°i
					await redis.hset('worker:status', chosenWorker, 'busy');
				} else {
					// Kh√¥ng c√≥ worker n√†o s·∫µn s√†ng, ch·ªù 100ms r·ªìi th·ª≠ l·∫°i
					await new Promise((r) => setTimeout(r, 100));
				}
			}

			// L·∫•y partition cho worker
			const partitionRaw = await redis.hget('worker:partition', chosenWorker);
			if (!partitionRaw) {
				console.warn(`‚ö†Ô∏è No partition assigned to ${chosenWorker}`);
				// Tr·∫£ worker v·ªÅ tr·∫°ng th√°i s·∫µn s√†ng r·ªìi b·ªè qua chunk n√†y
				await redis.hset('worker:status', chosenWorker, '1');
				return;
			}

			let partitions;
			try {
				partitions = JSON.parse(partitionRaw);
			} catch (err) {
				console.error(`‚ùå Error parsing partitions for ${chosenWorker}:`, err);
				await redis.hset('worker:status', chosenWorker, '1');
				return;
			}

			if (!Array.isArray(partitions) || partitions.length === 0) {
				console.warn(`‚ö†Ô∏è Invalid partition list for ${chosenWorker}`);
				await redis.hset('worker:status', chosenWorker, '1');
				return;
			}

			// Ch·ªçn ng·∫´u nhi√™n 1 partition
			const partition =
				partitions[Math.floor(Math.random() * partitions.length)];

			try {
				// G·ª≠i chunk l√™n Kafka
				await runProducer(chosenWorker, chunkId, chunk, partition);

				console.log(
					`‚úÖ Sent batch ${chunkId} to ${chosenWorker} via partition ${partition} of topic ${process.env.KAFKA_TOPIC_NAME_MASTER}`
				);
			} catch (err) {
				console.error(
					`‚ùå Failed to assign chunk ${chunkId} to ${chosenWorker}:`,
					err
				);
			} finally {
				// D√π th√†nh c√¥ng hay l·ªói, tr·∫£ worker v·ªÅ l·∫°i '1' (s·∫µn s√†ng)
				await redis.hset('worker:status', chosenWorker, '1');
			}
		});

		// ƒê·ª£i to√†n b·ªô c√°c chunk c·ªßa file hi·ªán t·∫°i xong m·ªõi sang file k·∫ø ti·∫øp
		await Promise.all(chunkPromises);
		console.log(`‚úÖ Ho√†n th√†nh file: ${currentBatchFile}\n`);
		// ------------- Ghi log ra file "processed_batches.log" -------------
		try {
			fs.appendFileSync(
				'processed_batches.log',
				`${new Date().toISOString()} - Processed file: ${currentBatchFile}\n`,
				'utf-8'
			);
			console.log(
				`üìù ƒê√£ ghi log file "${currentBatchFile}" v√†o processed_batches.log`
			);
		} catch (err) {
			console.error('‚ùå L·ªói khi ghi log processed_batches.log:', err);
		}
	}
	console.log('‚úÖ T·∫•t c·∫£ file trong batchQueue ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω!');
};

const runProducer = async (workerId, batchId, ipList, partition) => {
	await producer.connect();
	await redis.hset('worker:total', batchId, ipList.length);
	try {
		await producer.send({
			topic: process.env.KAFKA_TOPIC_NAME_MASTER,
			messages: [
				{
					key: workerId,
					partition,
					value: JSON.stringify({
						id: workerId,
						batchId,
						data: ipList,
					}),
				},
			],
		});
		console.log(
			`‚úÖ Sent batch ${batchId} to ${workerId} via partition ${partition} of topic ${process.env.KAFKA_TOPIC_NAME_MASTER}`
		);
	} catch (err) {
		console.error(`‚ùå Failed to send to ${workerId}:`, err);
		throw err;
	}

	await producer.disconnect();
};

export default runProducer;
