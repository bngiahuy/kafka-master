import 'dotenv/config';
import kafka from '../configs/kafkaConfig.js';
import fs from 'fs';
import path from 'path';
import redis from '../configs/redisConfig.js';
import { randomUUID } from 'crypto';
import logMessage from '../utils/logger.js';
const producer = kafka.producer();

const BATCH_DIR = './input_ip_data';
const BATCH_FILES = fs.readdirSync(BATCH_DIR).filter((f) => f.endsWith('.txt'));
let batchQueue = [...BATCH_FILES];

const acquireLock = async (workerId, timeout = 1000) => {
	const lockKey = `lock:worker:${workerId}`;
	const startTime = Date.now();
	const result = await redis.set(lockKey, 'locked', 'NX', 'PX', timeout);
	const endTime = Date.now();
	console.log(
		`üöÄ ~ acquireLock ~ worker: ${workerId}, result: ${result}, time: ${
			endTime - startTime
		}ms`
	);
	return result === 'OK';
};

const releaseLock = async (workerId) => {
	const lockKey = `lock:worker:${workerId}`;
	await redis.del(lockKey);
};

export const assignBatches = async () => {
	// Ti·∫øp t·ª•c x·ª≠ l√Ω cho ƒë·∫øn khi h√†ng ƒë·ª£i file tr·ªëng
	while (batchQueue.length > 0) {
		// --- L·∫•y file ti·∫øp theo t·ª´ h√†ng ƒë·ª£i ---
		const currentBatchFile = batchQueue.shift();
		const batchIdBase = path.basename(currentBatchFile, '.txt') || randomUUID();
		const filePath = path.join(BATCH_DIR, currentBatchFile);
		const lines = fs
			.readFileSync(filePath, 'utf-8')
			.split('\n')
			.filter((line) => line.trim());

		// --- L·∫•y chunkSize t·ª´ Redis (m·∫∑c ƒë·ªãnh 1000 n·∫øu parse l·ªói) ---
		const chunkSizeRaw = await redis.get('numBatches');
		// console.log('üöÄ ~ assignBatches ~ chunkSizeRaw:', chunkSizeRaw);
		const chunkSize = parseInt(chunkSizeRaw) || 1000;

		// --- Chia file th√†nh c√°c chunk ---
		const fileChunks = [];
		for (let i = 0; i < lines.length; i += chunkSize) {
			const chunk = lines.slice(i, i + chunkSize);
			const chunkId = `${batchIdBase}_${i / chunkSize}`;
			fileChunks.push({ chunkId, chunk });
		}

		console.log(`\nB·∫Øt ƒë·∫ßu x·ª≠ l√Ω file: ${currentBatchFile}`);
		console.log(`  -> T·ªïng s·ªë chunk: ${fileChunks.length}`);

		// --- G·ª≠i song song c√°c chunk cho c√°c worker s·∫µn s√†ng ---
		const chunkPromises = fileChunks.map(async ({ chunkId, chunk }) => {
			let chosenWorker = null;
			do {
				// L·∫•y danh s√°ch worker v√† tr·∫°ng th√°i
				const allWorkers = await redis.hgetall('worker:status');
				// L·ªçc ra c√°c worker s·∫µn s√†ng
				const readyWorkers = Object.entries(allWorkers)
					.filter(([_, status]) => status === '1')
					.map(([id]) => id);

				if (readyWorkers.length > 0) {
					// Th·ª≠ chi·∫øm kh√≥a cho t·ª´ng worker s·∫µn s√†ng
					for (const worker of readyWorkers) {
						console.log('üöÄ ~ chunkPromises ~ worker:', worker);
						if (await acquireLock(worker)) {
							chosenWorker = worker;

							// ƒê·∫∑t tr·∫°ng th√°i worker th√†nh "busy"
							await redis.hset('worker:status', chosenWorker, '0');
							const currentWorkerStatus = await redis.hget(
								'worker:status',
								chosenWorker
							);
							console.log(
								`üöÄ ~ ${chosenWorker} ~ currentWorkerStatus: ${currentWorkerStatus} `
							);
							break;
						}
					}
				}

				if (!chosenWorker) {
					// Kh√¥ng c√≥ worker n√†o s·∫µn s√†ng ho·∫∑c kh√¥ng chi·∫øm ƒë∆∞·ª£c kh√≥a, ch·ªù 100ms r·ªìi th·ª≠ l·∫°i
					await new Promise((r) => setTimeout(r, 100));
				}
			} while (!chosenWorker);

			// L·∫•y partition cho worker
			const partitionRaw = await redis.hget('worker:partition', chosenWorker);
			if (!partitionRaw) {
				console.warn(`‚ö†Ô∏è No partition assigned to ${chosenWorker}`);
				await releaseLock(chosenWorker);
				await redis.hset('worker:status', chosenWorker, '1');
				return;
			}

			let partitions;
			try {
				partitions = JSON.parse(partitionRaw);
			} catch (err) {
				console.error(`‚ùå Error parsing partitions for ${chosenWorker}:`, err);
				await releaseLock(chosenWorker);
				await redis.hset('worker:status', chosenWorker, '1');
				return;
			}

			if (!Array.isArray(partitions) || partitions.length === 0) {
				console.warn(`‚ö†Ô∏è Invalid partition list for ${chosenWorker}`);
				await releaseLock(chosenWorker);
				await redis.hset('worker:status', chosenWorker, '1');
				return;
			}

			// Ch·ªçn ng·∫´u nhi√™n 1 partition
			const partition =
				partitions[Math.floor(Math.random() * partitions.length)];

			try {
				// G·ª≠i chunk l√™n Kafka
				await runProducer(chosenWorker, chunkId, chunk, partition);
				console.log(
					`‚úÖ Sent batch ${chunkId} to ${chosenWorker} via partition ${partition} of topic ${process.env.KAFKA_TOPIC_NAME_MASTER}`
				);
			} catch (err) {
				console.error(
					`‚ùå Failed to assign chunk ${chunkId} to ${chosenWorker}:`,
					err
				);
			} finally {
				// Gi·∫£i ph√≥ng kh√≥a v√† tr·∫£ worker v·ªÅ tr·∫°ng th√°i s·∫µn s√†ng
				await releaseLock(chosenWorker);
				await redis.hset('worker:status', chosenWorker, '1');
			}
		});

		// ƒê·ª£i to√†n b·ªô c√°c chunk c·ªßa file hi·ªán t·∫°i xong m·ªõi sang file k·∫ø ti·∫øp
		await Promise.all(chunkPromises);
		console.log(`‚úÖ Ho√†n th√†nh file: ${currentBatchFile}\n`);

		// --- Ghi log ra file "processed_batches.log" ---
		try {
			logMessage('Batch ƒë√£ x·ª≠ l√Ω: ' + currentBatchFile);
			logMessage(
				`üìù ƒê√£ ghi log file "${currentBatchFile}" v√†o processed_batches.log`
			);
		} catch (err) {
			console.error('‚ùå L·ªói khi ghi log processed_batches.log:', err);
		}
	}
	console.log('‚úÖ T·∫•t c·∫£ file trong batchQueue ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω!');
};

const runProducer = async (workerId, batchId, ipList, partition) => {
	await producer.connect();
	await redis.hset('worker:total', batchId, ipList.length);
	try {
		await producer.send({
			topic: process.env.KAFKA_TOPIC_NAME_MASTER,
			messages: [
				{
					key: workerId,
					partition,
					value: JSON.stringify({
						id: workerId,
						batchId,
						data: ipList,
					}),
				},
			],
		});
		console.log(
			`‚úÖ Sent batch ${batchId} to ${workerId} via partition ${partition} of topic ${process.env.KAFKA_TOPIC_NAME_MASTER}`
		);
	} catch (err) {
		console.error(`‚ùå Failed to send to ${workerId}:`, err);
		throw err;
	}
	await producer.disconnect();
};

export default runProducer;
