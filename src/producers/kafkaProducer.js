import 'dotenv/config';
import kafka from '../configs/kafkaConfig.js';
import fs from 'fs';
import path from 'path';
import redis from '../configs/redisConfig.js';
import { logMessage } from '../utils/logger.js';
import { acquireLock, releaseLock } from '../utils/helper.js';

const producer = kafka.producer();

const DATA_DIR = process.env.CLIENT_DATA_PATH + '/input';
const PROCESSED_FILES_KEY = 'processed:files'; // Key cho Redis Set
const CHECK_INTERVAL = 5000; // Ki·ªÉm tra file m·ªõi m·ªói 5 gi√¢y (5000ms)
const WORKER_POLL_INTERVAL = 100;
let isRunning = true; // Bi·∫øn c·ªù ƒë·ªÉ d·ª´ng v√≤ng l·∫∑p ch√≠nh

// --- Helper function ƒë·ªÉ ch·ªù ---
const delay = (ms) => new Promise((resolve) => setTimeout(resolve, ms));

// --- H√†m ƒë·ªçc v√† l·ªçc file m·ªõi ---
const getNewFiles = async () => {
	try {
		const allFilesInDir = fs
			.readdirSync(DATA_DIR)
			.filter((f) => f.endsWith('.txt'));
		const processedFiles = await redis.smembers(PROCESSED_FILES_KEY);
		const processedFilesSet = new Set(processedFiles); // Chuy·ªÉn sang Set ƒë·ªÉ ki·ªÉm tra nhanh h∆°n
		const newFiles = allFilesInDir.filter(
			(file) => !processedFilesSet.has(file)
		);
		return newFiles;
	} catch (error) {
		console.error('‚ùå Error reading directory or Redis for new files:', error);
		return []; // Tr·∫£ v·ªÅ m·∫£ng r·ªóng n·∫øu c√≥ l·ªói
	}
};

// --- H√†m g√°n batch ch√≠nh, gi·ªù ch·∫°y trong v√≤ng l·∫∑p ---
export const startBatchAssigner = async () => {
	console.log('üöÄ Starting batch assignment loop...');
	try {
		await producer.connect();
		console.log('‚úÖ Kafka Producer connected.');
	} catch (err) {
		console.error('‚ùå Failed to connect Kafka Producer:', err);
		return; // Kh√¥ng th·ªÉ ti·∫øp t·ª•c n·∫øu kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c producer
	}

	while (isRunning) {
		const fileQueue = await getNewFiles(); // L·∫•y danh s√°ch file m·ªõi c·∫ßn x·ª≠ l√Ω
		if (fileQueue.length === 0) {
			console.log(
				`üïí No new files found. Waiting ${CHECK_INTERVAL / 1000}s...`
			);
			process.stdout.write(' W '); // Waiting symbol
			await delay(CHECK_INTERVAL);
			continue; // Quay l·∫°i ƒë·∫ßu v√≤ng l·∫∑p ƒë·ªÉ ki·ªÉm tra l·∫°i
		}

		process.stdout.write('\n'); // New line when files found
		console.log(
			`üìÇ Found ${fileQueue.length} new files to process:`,
			fileQueue
		);

		// ---- B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√°c file trong fileQueue hi·ªán t·∫°i ----
		while (fileQueue.length > 0 && isRunning) {
			// Th√™m ki·ªÉm tra isRunning ·ªü ƒë√¢y n·ªØa
			const currentFile = fileQueue.shift(); // L·∫•y v√† x√≥a file kh·ªèi h√†ng ƒë·ª£i
			if (!currentFile) continue;

			console.log(`\n‚è≥ Processing file: ${currentFile}`);
			console.log('Remaining in current queue: ', fileQueue);

			const fileIdBase =
				path.basename(currentFile, '.txt');
			const filePath = path.join(DATA_DIR, currentFile);
			let lines;
			try {
				lines = fs
					.readFileSync(filePath, 'utf-8')
					.split('\n')
					.filter((line) => line.trim());
			} catch (err) {
				console.error(`‚ùå Error reading file ${currentFile}:`, err);
				logMessage(`Error reading file ${currentFile}: ${err.message}`);
				continue; // B·ªè qua file l·ªói n√†y trong h√†ng ƒë·ª£i hi·ªán t·∫°i
			}

			if (lines.length === 0) {
				console.warn(
					`‚ö†Ô∏è File ${currentFile} is empty. Marking as processed.`
				);
				logMessage(`Skipped empty file: ${currentFile}`);
				await redis.sadd(PROCESSED_FILES_KEY, currentFile);
				continue;
			}

			// --- Logic chia chunk v√† g√°n worker (gi·ªØ nguy√™n ph·∫ßn l·ªõn) ---
			const chunkSizeRaw = await redis.get('numBatches');
			const chunkSize = parseInt(chunkSizeRaw) || 500;
			if (await redis.llen('master:fileChunks') === 0) {
				// N·∫øu fileChunks r·ªóng th√¨ chia chunk v√† g√°n worker
				for (let i = 0; i < lines.length; i += chunkSize) {
					const chunk = lines.slice(i, i + chunkSize);
					const chunkIndex = Math.floor(i / chunkSize);
					const chunkId = `${fileIdBase}_chunk_${chunkIndex}`;
					await redis.rpush('master:fileChunks', JSON.stringify({ chunkId, chunk }));
				}
			}
			// N·∫øu fileChunks kh√¥ng r·ªóng th√¨ ti·∫øp t·ª•c x·ª≠ l√Ω 
			console.log(`  -> Total Chunks: ${await redis.llen('master:fileChunks')}`);

			let fileProcessingSuccess = true; // C·ªù ƒë√°nh d·∫•u file x·ª≠ l√Ω th√†nh c√¥ng

			while ((await redis.llen('master:fileChunks')) > 0 && isRunning) {
				let chosenWorker = null;
				let lockedWorkerId = null;

				const allWorkers = await redis.hgetall('worker:status');
				const readyWorkers = Object.entries(allWorkers)
					.filter(([_, status]) => status === '1')
					.map(([id]) => id);

				if (readyWorkers.length === 0) {
					process.stdout.write('.'); // Waiting for worker
					await delay(WORKER_POLL_INTERVAL);
					if (!isRunning) break; // Tho√°t n·∫øu nh·∫≠n t√≠n hi·ªáu d·ª´ng trong l√∫c ƒë·ª£i worker
					continue;
				}

				for (const workerId of readyWorkers) {
					if (!isRunning) break; // Tho√°t s·ªõm n·∫øu nh·∫≠n t√≠n hi·ªáu d·ª´ng
					try {
						if (await acquireLock(workerId, 5000)) {
							lockedWorkerId = workerId;
							const currentStatus = await redis.hget('worker:status', workerId);
							if (currentStatus === '1') {
								chosenWorker = workerId;
								console.log(`   üîí Locked worker ${chosenWorker}`);
								break;
							} else {
								console.warn(
									`   ‚ö†Ô∏è Worker ${workerId} status changed. Releasing lock.`
								);
								await releaseLock(workerId);
							}
						}
					} catch (error) {
						console.error(`Error processing worker ${workerId}:`, error);
					}
				}
				if (!isRunning) break; // Tho√°t v√≤ng l·∫∑p g√°n chunk n·∫øu nh·∫≠n t√≠n hi·ªáu d·ª´ng

				if (chosenWorker) {
					const chunkToAssign = await redis.lpop('master:fileChunks');
					if (!chunkToAssign) {
						console.error('   ‚ùå Logic error: No more chunks to assign.');
						if (lockedWorkerId) await releaseLock(lockedWorkerId);
						fileProcessingSuccess = false; // ƒê√°nh d·∫•u file th·∫•t b·∫°i
						break;
					}
					const { chunkId, chunk } = JSON.parse(chunkToAssign);
					await redis.hset('worker:status', chosenWorker, '0');
					console.log(`   üö¶ Marked worker ${chosenWorker} as busy (0)`);

					const partitionRaw = await redis.hget(
						'worker:partition',
						chosenWorker
					);
					let partitions;
					try {
						if (!partitionRaw) throw new Error('No partition info found');
						partitions = JSON.parse(partitionRaw);
						if (!Array.isArray(partitions) || partitions.length === 0) {
							throw new Error('Invalid partition list');
						}
					} catch (err) {
						console.error(
							`   ‚ùå Partition error for ${chosenWorker}: ${err.message}`
						);
						await redis.hset('worker:status', chosenWorker, '0');
						if (lockedWorkerId) await releaseLock(lockedWorkerId);
						console.log(
							`   üîß Reset worker ${chosenWorker} to ready (1) and released lock.`
						);
						await redis.lpush('master:fileChunks', chunkToAssign);
						chosenWorker = null;
						await delay(500);
						continue; // Th·ª≠ t√¨m worker kh√°c
					}

					// const partition = partitions[Math.floor(Math.random() * partitions.length)];
					const partition = partitions.at(0);
					try {
						console.log(
							`   üì¶ Assigning chunk ${chunkId} (${chunk.length} items) to ${chosenWorker} (partition ${partition})`
						);
						await redis.hset(
							'worker:batchInfo',
							chosenWorker,
							JSON.stringify({
								batchId: chunkId,
								total: chunk.length,
								assignedAt: Date.now(),
							})
						);
						await sendChunkToKafka(chosenWorker, chunkId, chunk, partition);
						console.log(`   üîÑ Assigned chunk ${chunkId} to ${chosenWorker}`);

						// TH√äM M·ªöI: Ki·ªÉm tra v√† √°p d·ª•ng c·∫≠p nh·∫≠t ƒëang ch·ªù
						const needsUpdate = await redis.exists(`worker:needs:update:${chosenWorker}`);
						if (needsUpdate) {
							console.log(`‚ö†Ô∏è Worker ${chosenWorker} has pending updates that will be applied later`);
						}
					} catch (err) {
						console.error(
							`   ‚ùå Send/Assign error for chunk ${chunkId} to ${chosenWorker}:`,
							err
						);
						logMessage(
							`Failed assign/send chunk ${chunkId} to ${chosenWorker}: ${err.message}`
						);
						await redis.hset('worker:status', chosenWorker, '0');
						if (lockedWorkerId) await releaseLock(lockedWorkerId);
						console.log(
							`   üîß Reset worker ${chosenWorker} to ready (1) and released lock due to send error.`
						);
						await redis.lpush('master:fileChunks', chunkToAssign);
						await delay(500); // Ch·ªù ch√∫t tr∆∞·ªõc khi th·ª≠ l·∫°i
					}
				} else {
					process.stdout.write('~'); // Waiting for lock
					await delay(100);
					if (!isRunning) break; // Tho√°t n·∫øu nh·∫≠n t√≠n hi·ªáu d·ª´ng trong l√∫c ƒë·ª£i lock
				}
			} // --- K·∫øt th√∫c v√≤ng l·∫∑p g√°n chunk cho file hi·ªán t·∫°i ---

			if (!isRunning) break; // Tho√°t v√≤ng l·∫∑p x·ª≠ l√Ω file n·∫øu nh·∫≠n t√≠n hi·ªáu d·ª´ng

			// --- ƒê√°nh d·∫•u file ƒë√£ x·ª≠ l√Ω v√†o Redis SET ---
			if ((await redis.llen('master:fileChunks')) === 0 && fileProcessingSuccess) {
				try {
					await redis.sadd(PROCESSED_FILES_KEY, currentFile);
					console.log(
						`üíæ Marked file "${currentFile}" as processed in Redis.`
					);
					logMessage('Batch file marked as processed: ' + currentFile);
				} catch (redisErr) {
					console.error(
						`‚ùå Error marking file "${currentFile}" as processed in Redis:`,
						redisErr
					);
					// C√¢n nh·∫Øc: N·∫øu l·ªói ·ªü ƒë√¢y, file n√†y c√≥ th·ªÉ b·ªã x·ª≠ l√Ω l·∫°i l·∫ßn sau.
				}
			} else if (!fileProcessingSuccess) {
				console.error(
					`‚ùå File "${currentFile}" processing failed. It will be retried later.`
				);
				logMessage(`File "${currentFile}" processing failed. Will retry.`);
			} else {
				console.warn(
					`‚ö†Ô∏è File "${currentFile}" processing interrupted. It will be retried later.`
				);
				logMessage(
					`File "${currentFile}" processing interrupted. Will retry.`
				);
			}
		} // --- K·∫øt th√∫c v√≤ng l·∫∑p x·ª≠ l√Ω c√°c file trong fileQueue hi·ªán t·∫°i ---
	} // --- K·∫øt th√∫c v√≤ng l·∫∑p ch√≠nh (khi isRunning = false) ---

	console.log('üõë Batch assignment loop stopped.');
	try {
		await producer.disconnect();
		console.log('üîå Kafka Producer disconnected.');
	} catch (err) {
		console.error('‚ùå Error disconnecting Kafka Producer:', err);
	}
};

const sendChunkToKafka = async (workerId, batchId, ipList, partition) => {
	try {
		await producer.send({
			topic: process.env.KAFKA_TOPIC_NAME_MASTER,
			messages: [
				{
					key: workerId,
					partition: partition,
					value: JSON.stringify({
						id: workerId,
						batchId: batchId,
						data: ipList,
					}),
				},
			],
		});
	} catch (err) {
		console.error(
			`   ‚ùå Kafka send error for batch ${batchId} to worker ${workerId}:`,
			err
		);
		throw err; // N√©m l·ªói ƒë·ªÉ h√†m g·ªçi x·ª≠ l√Ω
	}
};

// --- X·ª≠ l√Ω t√≠n hi·ªáu d·ª´ng ---
const handleShutdown = async (signal) => {
	console.log(`\n‚úã ${signal} received. Stopping batch assigner loop...`);
	isRunning = false;
	// Kh√¥ng c·∫ßn g·ªçi disconnect ·ªü ƒë√¢y, v√≤ng l·∫∑p s·∫Ω t·ª± tho√°t v√† disconnect
};

process.on('SIGTERM', () => handleShutdown('SIGTERM'));
process.on('SIGINT', () => handleShutdown('SIGINT')); // Ctrl+C
